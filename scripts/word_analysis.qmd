---
title: "Preparing Word Analysis"
author: "Tiffany Sun"
date: "2025-04-24"
format: html
---

The purpose of this file is to wrangle the dataset to prepare for word analysis.
Here I have created two datasets: one that enables analysis at a bigger picture
ana another that enables analysis per game. 

```{r}
#| label: setup
#| include: false

knitr::opts_chunk$set(eval = FALSE, message = FALSE)

# improve digit and NA display 
options(scipen = 1, knitr.kable.NA = '')

# load packages
library(tidyverse)
library(tidytext)
library(parsedate)

```

```{r}
#| label: read-files

# Read clean game data. 
games_data <- readRDS("../data/processed/clean_games.rds")

# Parsed date to obtain year. 
games_data <- games_data |>
mutate(release_date = parse_date(release_date),
       year = year(release_date))

# Removed numeric values. 
games_description <- games_data |>
  select(name, about_the_game, year) |>
  mutate(
    about_the_game = str_replace_all(about_the_game, "[:digit:]", "")
  )

```

```{r}
#| label: Dataset for whole STEAM text analysis

#Retrieved AFINN dataset
afinn_lexicon <- get_sentiments("afinn")

# Creating a word dataset that allows analysis as a whole
# Has variables: name, word, n (count)
total_games_words <- games_description |>
  unnest_tokens(output = word, input = about_the_game) |>
  anti_join(stop_words, by = "word") |>
  count(word, sort = TRUE) 

# Added to include sentiment values per word
total_games_words <-
   mutate(
    left_join(total_games_words, afinn_lexicon)
  ) |>
  na.omit()

```

```{R}
#| Datasets for per-game text analysis and sentiment analysis
# Creating a word dataset that allows analysis per game
# Has variables: name, word, n (count)
grouped_games_words <- games_description |>
  unnest_tokens(output = word, input = about_the_game) |>
  anti_join(stop_words, by = "word") |>
  group_by(name) |>
  count(word, sort = TRUE) 

# Created another dataset to include sentiment values per word. 
# This will cause exclusion of more words
grouped_games_sentiments <- 
  mutate(
    left_join(grouped_games_words, afinn_lexicon)
  ) |>
  na.omit()
```

```{R}
#| Label: Time Analysis Datasets 

# Creating a word dataset that allows analysis across time
# Has variables: name, word, n (count), and year
sentimentvstime <- games_description |>
  unnest_tokens(output = word, input = about_the_game) |>
  anti_join(stop_words, by = "word") |>
  group_by(year) |>
  count(word, sort = TRUE)

# Added to include sentiment values per word
sentimentvstime <- 
   mutate(
    left_join(sentimentvstime, afinn_lexicon)
  ) |>
  na.omit()

# Create a numerical vector with no duplicates. 
years <- unique(sentimentvstime$year)

# Creates a function to filter out a specific component of the dataset
topten <- function(y) {
  sentimentvstime |>
    filter(year == y) |>
    arrange(desc(n)) 
}

# Goes through the numerical vector of years, if the year is later than 2000
# and is divisible by 5, a dynamic variable is created, using the function
# to create a top ten most common word dataset.
for (i in 1:length(years)) {
  if (years[[i]] %% 5 == 0 & 
      years[[i]] > 2000) {
  assign(paste0("year_", years[[i]]), topten((years[[i]])) |>
    slice(1:10) )}
}

```


```{r}
#| label: saving-data

# data used for word analysis
saveRDS(total_games_words, file = "../data/processed/total_games_words.RDS")
saveRDS(year_2025, file = "../data/processed/year_2025.RDS")
saveRDS(year_2020, file = "../data/processed/year_2020.RDS")
saveRDS(year_2015, file = "../data/processed/year_2015.RDS")
saveRDS(year_2010, file = "../data/processed/year_2010.RDS")
saveRDS(year_2005, file = "../data/processed/year_2005.RDS")
saveRDS(grouped_games_words, file = "../data/processed/grouped_games_words.RDS")
saveRDS(grouped_games_sentiments, file = "../data/processed/grouped_games_sentiments.RDS")
```
